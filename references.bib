@inproceedings{Dayan1992FeudalRL,
  title={Feudal Reinforcement Learning},
  author={P. Dayan and Geoffrey E. Hinton},
  booktitle={NIPS},
  year={1992}
}

@article{Vezhnevets2016StrategicAW,
  title={Strategic Attentive Writer for Learning Macro-Actions},
  author={Alexander Vezhnevets and Volodymyr Mnih and Simon Osindero and Alex Graves and Oriol Vinyals and John Agapiou and Koray Kavukcuoglu},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.04695}
}

@article{Kulkarni2016HierarchicalDR,
  title={Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
  author={Tejas D. Kulkarni and Karthik Narasimhan and A. Saeedi and J. Tenenbaum},
  journal={ArXiv},
  year={2016},
  volume={abs/1604.06057}
}

@article{Nachum2018DataEfficientHR,
  title={Data-Efficient Hierarchical Reinforcement Learning},
  author={Ofir Nachum and Shixiang Gu and H. Lee and S. Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.08296}
}

@inproceedings{Mordatch2018EmergenceOG,
  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},
  author={Igor Mordatch and P. Abbeel},
  booktitle={AAAI},
  year={2018}
}

@article{Wang2004ImageQA,
  title={Image quality assessment: from error visibility to structural similarity},
  author={Z. Wang and A. Bovik and H. R. Sheikh and E. P. Simoncelli},
  journal={IEEE Transactions on Image Processing},
  year={2004},
  volume={13},
  pages={600-612}
}

@article{Mnih2013PlayingAW,
  title={Playing Atari with Deep Reinforcement Learning},
  author={V. Mnih and K. Kavukcuoglu and D. Silver and A. Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},
  journal={ArXiv},
  year={2013},
  volume={abs/1312.5602}
}

@article{Sharma2020DynamicsAwareUD,
  title={Dynamics-Aware Unsupervised Discovery of Skills},
  author={Archit Sharma and Shixiang Gu and S. Levine and V. Kumar and Karol Hausman},
  journal={ArXiv},
  year={2020},
  volume={abs/1907.01657}
}

@article{levy2017learning,
  title={Learning multi-level hierarchies with hindsight},
  author={Levy, Andrew and Konidaris, George and Platt, Robert and Saenko, Kate},
  journal={arXiv preprint arXiv:1712.00948},
  year={2017}
}

@article{Schrittwieser2020MasteringAG,
  title={Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  author={Julian Schrittwieser and Ioannis Antonoglou and T. Hubert and K. Simonyan and L. Sifre and S. Schmitt and A. Guez and Edward Lockhart and Demis Hassabis and T. Graepel and T. Lillicrap and D. Silver},
  journal={Nature},
  year={2020},
  volume={588 7839},
  pages={
          604-609
        }
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@inproceedings{choi_contingency-aware_2019,
	title = {Contingency-{Aware} {Exploration} in {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=HyxGB2AcY7},
	abstract = {This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. For example, we report a state-of-the-art score of {\textgreater}11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.},
	urldate = {2021-03-11},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Choi, Jongwook and Guo, Yijie and Moczulski, Marcin and Oh, Junhyuk and Wu, Neal and Norouzi, Mohammad and Lee, Honglak},
	month = mar,
	year = {2019},
	note = {arXiv: 1811.01483},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, study},
	annote = {Comment: In ICLR 2019}
}

@inproceedings{badia_never_2020,
	title = {Never {Give} {Up}: {Learning} {Directed} {Exploration} {Strategies}},
	shorttitle = {Never {Give} {Up}},
	url = {https://openreview.net/forum?id=Sye57xStvB},
	abstract = {We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators (UVFA) to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0\%. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Badia, Adrià Puigdomènech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Martín and Pritzel, Alexander and Bolt, Andew and Blundell, Charles},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06038},
	keywords = {Computer Science - Machine Learning, GREAT, interesting, read, Statistics - Machine Learning, study},
	annote = {Comment: Published as a conference paper in ICLR 2020}
}

@inproceedings{pathak_curiosity-driven_2017,
	title = {Curiosity-driven {Exploration} by {Self}-supervised {Prediction}},
	url = {http://proceedings.mlr.press/v70/pathak17a.html},
	abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to expl...},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2778--2787}
}

@inproceedings{burda_exploration_2019,
	title = {Exploration by random network distillation},
	url = {https://openreview.net/forum?id=H1lJJnR5Ym},
	abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access the underlying state of the game, and occasionally completes the first level. This suggests that relatively simple methods that scale well can be sufficient to tackle challenging exploration problems.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
	year = {2019},
	keywords = {interesting, study}
}


@incollection{watter_embed_2015,
	title = {Embed to {Control}: {A} {Locally} {Linear} {Latent} {Dynamics} {Model} for {Control} from {Raw} {Images}},
	shorttitle = {Embed to {Control}},
	url = {http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf},
	urldate = {2020-07-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2746--2754}
}


@inproceedings{dosovitskiy_you_2019,
	title = {You {Only} {Train} {Once}: {Loss}-{Conditional} {Training} of {Deep} {Networks}},
	shorttitle = {You {Only} {Train} {Once}},
	url = {https://openreview.net/forum?id=HyxY6JHKwr},
	abstract = {In many machine learning problems, loss functions are weighted sums of several terms. A typical approach to dealing with these is to train multiple separate models with different selections of weights and then either choose the best one according to some criterion or keep multiple models if it is desirable to maintain a diverse set of solutions. This is inefficient both at training and at inference time. We propose a method that allows replacing multiple models trained on one loss function each by a single model trained on a distribution of losses. At test time a model trained this way can be conditioned to generate outputs corresponding to any loss from the training distribution of losses. We demonstrate this approach on three tasks with parametrized losses: beta-VAE, learned image compression, and fast style transfer.},
	language = {en},
	urldate = {2021-03-18},
	author = {Dosovitskiy, Alexey and Djolonga, Josip},
	month = sep,
	year = {2019},
	keywords = {interesting}
}


@inproceedings{andrychowicz_hindsight_2017,
	title = {Hindsight {Experience} {Replay}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017}
}


@inproceedings{juliani_obstacle_2019,
	title = {Obstacle {Tower}: {A} {Generalization} {Challenge} in {Vision}, {Control}, and {Planning}},
	url = {https://doi.org/10.24963/ijcai.2019/373},
	doi = {10.24963/ijcai.2019/373},
	abstract = {The rapid pace of recent research in AI has been driven in part by the presence of fast and challenging simulation environments. These environments often take the form of games; with tasks ranging from simple board games, to competitive video games. We propose a new benchmark - Obstacle Tower: a high fidelity, 3D, 3rd person, procedurally generated environment. An agent in Obstacle Tower must learn to solve both low-level control and high-level planning problems in tandem while learning from pixels and a sparse reward signal. Unlike other benchmarks such as the Arcade Learning Environment, evaluation of agent performance in Obstacle Tower is based on an agent's ability to perform well on unseen instances of the environment. In this paper we outline the environment and provide a set of baseline results produced by current state-of-the-art Deep RL methods as well as human players. These algorithms fail to produce agents capable of performing near human level.},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-19},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Juliani, Arthur and Khalifa, Ahmed and Berges, Vincent-Pierre and Harper, Jonathan and Teng, Ervin and Henry, Hunter and Crespi, Adam and Togelius, Julian and Lange, Danny},
	year = {2019},
	pages = {2684--2691}
}


@inproceedings{ha_recurrent_2018,
	title = {Recurrent {World} {Models} {Facilitate} {Policy} {Evolution}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf},
	abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ha, David and Schmidhuber, Jürgen},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018}
}

@article{Kaiser2020ModelBasedRL,
  title={Model-Based Reinforcement Learning for Atari},
  author={Lukasz Kaiser and M. Babaeizadeh and P. Milos and B. Osinski and R. Campbell and K. Czechowski and D. Erhan and Chelsea Finn and Piotr Kozakowski and S. Levine and Ryan Sepassi and G. Tucker and H. Michalewski},
  journal={ArXiv},
  year={2020},
  volume={abs/1903.00374}
}

@inproceedings{Vezhnevets2017FeUdalNF,
  title={FeUdal Networks for Hierarchical Reinforcement Learning},
  author={A. Vezhnevets and Simon Osindero and Tom Schaul and N. Heess and Max Jaderberg and D. Silver and K. Kavukcuoglu},
  booktitle={ICML},
  year={2017}
}

@article{Bougie2019SkillbasedCF,
  title={Skill-based curiosity for intrinsically motivated reinforcement learning},
  author={Nicolas Bougie and R. Ichise},
  journal={Machine Learning},
  year={2019},
  volume={109},
  pages={493-512}
}

@inproceedings{
savinov2018semiparametric,
title={Semi-parametric topological memory for navigation},
author={Nikolay Savinov and Alexey Dosovitskiy and Vladlen Koltun},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SygwwGbRW},
}



@article{ecoffet_go-explore_2021,
	title = {Go-{Explore}: a {New} {Approach} for {Hard}-{Exploration} {Problems}},
	shorttitle = {Go-{Explore}},
	url = {http://arxiv.org/abs/1901.10995},
	abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).},
	urldate = {2021-03-11},
	journal = {arXiv:1901.10995 [cs, stat]},
	author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = feb,
	year = {2021},
	note = {arXiv: 1901.10995},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, queue},
	annote = {Comment: 37 pages, 14 figures; added references to Goyal et al. and Oh et al., updated reference to Colas et al; updated author emails; point readers to updated paper}
}

@article{ecoffet_first_2021,
	title = {First return, then explore},
	volume = {590},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03157-9},
	doi = {10.1038/s41586-020-03157-9},
	abstract = {Reinforcement learning promises to solve complex sequential-decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse1 and deceptive2 feedback. Avoiding these pitfalls requires a thorough exploration of the environment, but creating algorithms that can do so remains one of the central challenges of the field. Here we hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (detachment) and failing to first return to a state before exploring from it (derailment). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly ‘remembering’ promising states and returning to such states before intentionally exploring. Go-Explore solves all previously unsolved Atari games and surpasses the state of the art on all hard-exploration games1, with orders-of-magnitude improvements on the grand challenges of Montezuma’s Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore’s exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration—an insight that may prove critical to the creation of truly intelligent learning agents.},
	language = {en},
	number = {7847},
	urldate = {2021-04-01},
	journal = {Nature},
	author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	month = feb,
	year = {2021},
	note = {Number: 7847
Publisher: Nature Publishing Group},
	keywords = {queue},
	pages = {580--586}
}

@article{resnick_backplay_2018,
	title = {Backplay: "{Man} muss immer umkehren"},
	shorttitle = {Backplay},
	url = {http://arxiv.org/abs/1807.06919},
	abstract = {Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.},
	urldate = {2021-06-01},
	journal = {arXiv:1807.06919 [cs, stat]},
	author = {Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alexander and Cho, Kyunghyun and Bruna, Joan},
	month = dec,
	year = {2018},
	note = {arXiv: 1807.06919},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: AAAI-19 Workshop on Reinforcement Learning in Games}
}

@article{silver_reward_2021,
	title = {Reward {Is} {Enough}},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
	doi = {10.1016/j.artint.2021.103535},
	abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
	language = {en},
	urldate = {2021-05-31},
	journal = {Artificial Intelligence},
	author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
	month = may,
	year = {2021},
	keywords = {Artificial general intelligence, Artificial Intelligence, interesting, read, Reinforcement learning, reward, study},
	pages = {103535}
}

@article{salimans_learning_2018,
	title = {Learning {Montezuma}'s {Revenge} from a {Single} {Demonstration}},
	url = {http://arxiv.org/abs/1812.03381},
	abstract = {We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma's Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma's Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.},
	urldate = {2021-06-01},
	journal = {arXiv:1812.03381 [cs, stat]},
	author = {Salimans, Tim and Chen, Richard},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.03381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Deep RL Workshop, NeurIPS 2018}
}

@article{liu_learning_2018,
	title = {Learning {Abstract} {Models} for {Long}-{Horizon} {Exploration}},
	url = {https://openreview.net/forum?id=ryxLG2RcYX},
	abstract = {We automatically construct and explore a small abstract Markov Decision Process, enabling us to achieve state-of-the-art results on Montezuma's Revenge, Pitfall!, and Private Eye by a significant...},
	language = {en},
	urldate = {2021-06-01},
	author = {Liu, Evan Zheran and Keramati, Ramtin and Seshadri, Sudarshan and Guu, Kelvin and Pasupat, Panupong and Brunskill, Emma and Liang, Percy},
	month = sep,
	year = {2018}
}

@inproceedings{oh_self-imitation_2018,
	title = {Self-{Imitation} {Learning}},
	url = {http://proceedings.mlr.press/v80/oh18b.html},
	abstract = {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent’s past good decisions. This algorithm is designed to verify our hypo...},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {interesting, study},
	pages = {3878--3887}
}

@article{guo_self-imitation_2019,
	title = {Self-{Imitation} {Learning} via {Trajectory}-{Conditioned} {Policy} for {Hard}-{Exploration} {Tasks}},
	url = {http://arxiv.org/abs/1907.10247v2},
	abstract = {Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experience and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezuma's Revenge without using expert demonstrations or resetting to arbitrary states.},
	urldate = {2021-06-02},
	journal = {arXiv:1907.10247 [cs, stat]},
	author = {Guo, Yijie and Choi, Jongwook and Moczulski, Marcin and Bengio, Samy and Norouzi, Mohammad and Lee, Honglak},
	month = oct,
	year = {2019},
	note = {arXiv: 1907.10247
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{skrynnik_forgetful_2021,
	title = {Forgetful experience replay in hierarchical reinforcement learning from expert demonstrations},
	volume = {218},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001076},
	doi = {10.1016/j.knosys.2021.106844},
	abstract = {Deep reinforcement learning (RL) shows impressive results in complex gaming and robotic environments. These results are commonly achieved at the expense of huge computational costs and require an incredible number of episodes of interactions between the agent and the environment. Hierarchical methods and expert demonstrations are among the most promising approaches to improve the sample efficiency of reinforcement learning methods. In this paper, we propose a combination of methods that allow the agent to use low-quality demonstrations in complex vision-based environments with multiple related goals. Our Forgetful Experience Replay (ForgER) algorithm effectively handles expert data errors and reduces quality losses when adapting the action space and states representation to the agent’s capabilities. The proposed goal-oriented replay buffer structure allows the agent to automatically highlight sub-goals for solving complex hierarchical tasks in demonstrations. Our method has a high degree of versatility and can be integrated into various off-policy methods. The ForgER surpasses the existing state-of-the-art RL methods using expert demonstrations in complex environments. The solution based on our algorithm beats other solutions for the famous MineRL competition and allows the agent to demonstrate the behavior at the expert level.},
	language = {en},
	urldate = {2021-06-02},
	journal = {Knowledge-Based Systems},
	author = {Skrynnik, Alexey and Staroverov, Aleksey and Aitygulov, Ermek and Aksenov, Kirill and Davydov, Vasilii and Panov, Aleksandr I.},
	month = apr,
	year = {2021},
	keywords = {Expert demonstrations, ForgER, Goal-oriented reinforcement learning, Hierarchical reinforcement learning, Learning from demonstrations, Task-oriented augmentation},
	pages = {106844}
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2021-06-02},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning}
}

@inproceedings{kuttler_nethack_2020,
	title = {The {NetHack} {Learning} {Environment}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/569ff987c643b4bedf504efda8f786c2-Paper.pdf},
	abstract = {Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack. We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare NLE and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. NLE is open source at https://github.com/facebookresearch/nle.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Küttler, Heinrich and Nardelli, Nantas and Miller, Alexander and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rocktäschel, Tim},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, interesting, Statistics - Machine Learning},
	pages = {7671--7684},
	annote = {Comment: 28 pages. Accepted at NeurIPS 2020}
}

@article{hafner_dream_2020,
	title = {Dream to {Control}: {Learning} {Behaviors} by {Latent} {Imagination}},
	shorttitle = {Dream to {Control}},
	url = {http://arxiv.org/abs/1912.01603},
	abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
	urldate = {2021-04-29},
	journal = {arXiv:1912.01603 [cs]},
	author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.01603},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, interesting, study},
	annote = {Comment: 9 pages, 12 figures}
}


@article{hafner_mastering_2020,
	title = {Mastering {Atari} with {Discrete} {World} {Models}},
	url = {http://arxiv.org/abs/2010.02193},
	abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.},
	urldate = {2021-02-21},
	journal = {arXiv:2010.02193 [cs, stat]},
	author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.02193},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, interesting, Statistics - Machine Learning, study},
	annote = {Comment: 24 pages, 10 figures, 5 tables}
}

@inproceedings{hafner_learning_2019,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	url = {http://proceedings.mlr.press/v97/hafner19a.html},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the w...},
	language = {en},
	urldate = {2021-02-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {interesting, study},
	pages = {2555--2565}
}

@article{chen_decision_2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	shorttitle = {Decision {Transformer}},
	url = {http://arxiv.org/abs/2106.01345},
	abstract = {We present a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	urldate = {2021-06-06},
	journal = {arXiv:2106.01345 [cs]},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.01345},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: First two authors contributed equally. Last two authors advised equally}
}

@inproceedings{schulman_high-dimensional_2016,
	address = {San Juan, Puerto Rico},
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, study}
}

@inproceedings{espeholt_impala_2018,
	title = {{IMPALA}: {Scalable} {Distributed} {Deep}-{RL} with {Importance} {Weighted} {Actor}-{Learner} {Architectures}},
	shorttitle = {{IMPALA}},
	url = {http://proceedings.mlr.press/v80/espeholt18a.html},
	abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
	language = {en},
	urldate = {2021-06-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1407--1416}
}

@inproceedings{
savinov2018episodic,
title={Episodic Curiosity through Reachability},
author={Nikolay Savinov and Anton Raichuk and Damien Vincent and Raphael Marinier and Marc Pollefeys and Timothy Lillicrap and Sylvain Gelly},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkeK3s0qKQ},
}

@InProceedings{pmlr-v80-florensa18a,
  title = 	 {Automatic Goal Generation for Reinforcement Learning Agents},
  author =       {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1515--1528},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/florensa18a/florensa18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/florensa18a.html},
  abstract = 	 {Reinforcement learning (RL) is a powerful technique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to accomplish, each task being specified as reaching a certain parametrized subset of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent, thus automatically producing a curriculum. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment, even when only sparse rewards are available. Videos and code available at https://sites.google.com/view/goalgeneration4rl.}
}

@inproceedings{cobbe2020leveraging,
  title={Leveraging procedural generation to benchmark reinforcement learning},
  author={Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John},
  booktitle={International conference on machine learning},
  pages={2048--2056},
  year={2020},
  organization={PMLR}
}

@article{Yang2021ProgramSG,
  title={Program Synthesis Guided Reinforcement Learning},
  author={Yichen Yang and J. Inala and Osbert Bastani and Yewen Pu and Armando Solar-Lezama and M. Rinard},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.11137}
}

@article{Zhang2020BeBoldEB,
  title={BeBold: Exploration Beyond the Boundary of Explored Regions},
  author={Tianjun Zhang and Huazhe Xu and Xiaolong Wang and Yi Wu and K. Keutzer and Joseph Gonzalez and Y. Tian},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.08621}
}

@inproceedings{Shi2017WorldOB,
  title={World of Bits: An Open-Domain Platform for Web-Based Agents},
  author={Tianlin Shi and A. Karpathy and Linxi Fan and Jonathan Hernandez and Percy Liang},
  booktitle={ICML},
  year={2017}
}

@inproceedings{ALFRED20,
  title ={{ALFRED: A Benchmark for Interpreting Grounded
           Instructions for Everyday Tasks}},
  author={Mohit Shridhar and Jesse Thomason and Daniel Gordon and Yonatan Bisk and
          Winson Han and Roozbeh Mottaghi and Luke Zettlemoyer and Dieter Fox},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020},
  url  = {https://arxiv.org/abs/1912.01734}
}

@article{Raileanu2020RIDERI,
  title={RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments},
  author={Roberta Raileanu and Tim Rockt{\"a}schel},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.12292}
}

@inproceedings{Nair2018VisualRL,
  title={Visual Reinforcement Learning with Imagined Goals},
  author={Ashvin Nair and Vitchyr H. Pong and Murtaza Dalal and Shikhar Bahl and Steven Lin and Sergey Levine},
  booktitle={NeurIPS},
  year={2018}
}

@article{Zhao2021ACP,
  title={A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning},
  author={Mingde Zhao and Zhen Liu and Sitao Luan and Shuyuan Zhang and Doina Precup and Yoshua Bengio},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.02097}
}

@article{Carion2020EndtoEndOD,
  title={End-to-End Object Detection with Transformers},
  author={Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.12872}
}

@inproceedings{Ugadiarov2021LongTermEI,
  title={Long-Term Exploration in Persistent MDPs},
  author={Leonid Ugadiarov and Alexey Skrynnik and Aleksandr I. Panov},
  booktitle={MICAI},
  year={2021}
}

@inproceedings{Havrylov2017EmergenceOL,
  title={Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols},
  author={Serhii Havrylov and Ivan Titov},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{chen2019learning,
  title={Learning Effective Subgoals with Multi-Task Hierarchical Reinforcement Learning},
  author={Chen, Dagui and Yan, Qi and Guo, Shangqi and Yang, Zhile and Su, Xin and Chen, Feng},
  booktitle={Scaling-Up Reinforcement Learning (SURL) Workshop. URl: http://surl. tirl. info/proceedings/SURL-2019\_paper\_10. pdf},
  year={2019}
}