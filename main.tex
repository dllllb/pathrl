\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue
}
\newcommand{\ntt}[1]{{\color{red} [#1]}}
\usepackage[square,authoryear]{natbib}
%\usepackage{hyperref}
\usepackage[a4paper, margin=0.1\paperwidth]{geometry}

\title{State description language and PathRL}
\author{}
\date{}

\begin{document}

\maketitle

\section{PathRL algorithm}

\subsection{First phase}

The first phase of training can be described as a two-agent game: the encoder agent $E$ learns to produce a description of the task in some restricted inter-agent language, the navigator agent $N$ receives the task description and learns to execute the task. Both agents receive reward if navigator successfully executes the task. The tasks are defined as states that should be reached from some other states. The encoder agent takes desired state $s_g$ as its input and produces its description. The navigator takes description $E(s_g)$ from the encoder and current state $s_t$ and produces the next action: $a_t = N(s_t, E(s_g))$. The two-agent game is somewhat similar to the cooperative game proposed in~\citep{Mordatch2018EmergenceOG}.

The inter-agent language can be similar to the query language where the desired state is described as the set of predicates. In contrast to the exact algorithm of actions, the set of predicates is the declarative form of instructions.
% XXX not just the abstract controllable embedding of target state, but its high level description, generated based on it 

Generally, the task is a pair of queries which define the set of initial states and the set of states to reach. In current work we consider only the singleton sets of states, hence, the task is defined as a pair of states.
% XXX maybe the target query and a hint vector? as it seems possible that the worker-agent could stray arbitrarily far away from the destination on the embedded state manifold

The encoder does not know current state of the navigator during encoding, hence, it can't encode a set of instructions for the navigator agent and is forced to produce some declarative description of the desired state.

\subsubsection{Exploration for navigation tasks collection}

During the training time an agent performs exploration of the environment using some exploration policy $P_e$. Random trajectories of the exploration are then used to define the training tasks, hence, only pairs of states where one state is attainable from the other are used for training. Different types of exploration policy can be used in order to better explore the environment and collect diverse trajectories. One of the options is to use curiosity-based exploration policy \citep{Pathak2017CuriosityDrivenEB, Burda2019ExplorationBR}.

\subsubsection{Navigation reward}

One possible training scheme is to reward agents for moving in the direction of the target state $s_g$ from the current state $s_t$. Positive reward can be provided for the diminishing difference between $s_g$ and $s_t$. This removes the requirement to reach the exact target state which is not always possible in changing environments. In this case the function $d(s_a, s_b)$ that estimates the similarity between states is required. The state similarity function should also be transitive i. e. if $d(s_a, s_b) > d(s_a, s_c)$ then $d(s_b, s_c) > d(s_a, s_c)$.

Another scheme is to reward agent from reaching the state that is similar enough to the target state. In this case, the state similarity function is ton required to be transitive.

\subsubsection{State similarity function}

State similarity function $d(s_a, s_b)$ measures the similarity between the states. Some states, that are different, but that difference are not controlled by the agent should be considered equivalent. The example of the set of equivalent states would be states with different position of some oscillating pendulum. Hence, it is reasonable for the state similarity function to work on the state embeddings, where only the information, controllable by an agent is present, and other information is dropped. The state encoder, that drops information that are not controllable can be trained in self-supervised manner on the inverse dynamics task. I. e. the self-supervised  task is to predict the action from the embeddings of the pair of consecutive states.

\subsection{Second phase}

The second phase of training of this model is the addition of goal definition agent $G$ which replaces encoder agent $E$ after training. The goal definition agent receives only the current state and produces the set of predicates that could lead to the high reward: $e_g = G(s_t)$. I. e. the goal agent learns to generate the set of predicates that describe high-reward states. In the second phase, actions are produced using the following approach: $a_t = N(s_t, G(s_t))$. Navigator agent weights should be frozen during the second phase of training to force goal agent to use the same "language" of instructions as encoder agent.

\subsubsection{Reward scheme}

The combination of goal agent and frozen navigator agent are trained on the two rewards: extrinsic reward, i. e. true reward of the environment and intrinsic curiosity reward for effective exploration. In this scheme the goal agent is encouraged to propose both queries for states with high extrinsic rewards and queries for the states in the "unknown territories" with good possibility of curiosity reward. The combination of curiosity reward scheme and navigator agent have some parallels with Go-explore algorithm~\citep{Ecoffet2019GoExploreAN} where navigation is done via virtual environment resets.

\subsection{World state representations}

One of the options is to use encoded representations of the environment states instead of directly using environment states for encoder, navigator and goal definition agents. I. e. representation is the world state encoded by the world model pre-trained on the next state prediction task like in~\citep{Ha2018RecurrentWM}.

\subsection{Motivation}

The proposed scheme is effective for the environment with sparse rewards: there is no need to use sparse reward signals to learn basic navigation in the environment. Effective navigation skills can be learned without extrinsic reward using synthetic navigation task.

The proposed 2-step learning scheme is especially effective in the scenario of multi-task learning in the same environment. The navigator agent model, trained for the specific environment, can be effectively reused across different tasks.

As shown in \citep{Ecoffet2019GoExploreAN} the exploration of distant states greatly improves the algorithm effectiveness. The availability of navigator agent allows to train a goal agent that is encouraged to explore distant states. Unlike \citep{Ecoffet2019GoExploreAN} this scheme of agent training does not rely on some "hacks' like virtual environment reset.

\subsection{Path-RL vs model-based RL}

An RL task can be spited on three independent modules: world model, planner and goal selector. In model-based RL with Monte-Carlo tree search as in~\citep{Schrittwieser2020MasteringAG} planner and goal selector are merged into one module. In Path-RL we propose to merge world model and planner into one module.

\section{Related work}

\textbf{Feudal RL}~\citep{Dayan1992FeudalRL}

The task is to find a way in the maze. A hierarchy of policies is defined. A policy use blocks of space as atomic movement positions. Each level of hierarchy use exponentially larger blocks comparing to the previous level. A policy selects a desired block to move to as its action and then assigns a task to lower-level policy. Higher-level policy defines reward for the lower-level policy. The reward for lower-level policy is gained if it successfully guides the agent to the block, defined by the higher-level policy.

\textbf{STRAW}~\citep{Vezhnevets2016StrategicAW}

The network holds two structures in memory: action plan A and commitment plan c. T-th column of A contains probabilities to perform specific action at time t. c is a vector of probabilities to update the current plan at time t. If $g \sim c_{t-1} = 1$ then the network performs plan update. Both A and c are refreshed during update. The network is trained using A3C.

\textbf{Hierarchical Deep Reinforcement Learning}~\citep{Kulkarni2016HierarchicalDR}

The meta-controller learns an approximation of the optimal goal policy $\pi(g|s)$, i. e. it receives state s and select a goal g from the set of all possible current goals G. The meta-controller operates on the slower time-scale than the controller. The meta-controller receives extrinsic reward from the environment.

The controller learns an approximation of the optimal action policy $\pi(a|g,s)$, i. e. it receives state s and goal g and selects an action a. The internal critic provides intrinsic reward for the controller. For example, a critic for a game can check that specific conditions are fulfilled, e. g. an agent reaches the door.

The critic and the set of possible goals G are not learned but are considered available.

\textbf{HIRO}~\citep{Nachum2018DataEfficientHR}

Ant Gather / Ant Maze / Ant Push / Ant Fall tasks are used. The higher-level policy instructs the lower-level policy via high-level actions, or goals, which it samples anew every c steps. A higher-level policy takes current state and outputs the goal for lower-level policy. The lower-level policy interacts directly with the environment. The goal simply instructs the lower-level policy to reach the specific state. The reward for lower-level policy is provided if the environment yields an observation close to matching the desired goal.

\textbf{Curiosity-driven learing}~\citep{Pathak2017CuriosityDrivenEB}

Intrinsic Curiosity Module (ICM) predict next state $\hat{s}_{t+1}$ from the current state $s_t$ and action $a_t$ povided by policy network. Difference between predicted $\hat{s}_{t+1}$ and true $s_{t+1}$ is the intrinsic reward for the action $a_t$. Sum of reward from ICM and extrinistic reward is used to train policy network. ICM is in fact a world model.

\textbf{Random Network Distillation}

Another approach to the curiosity-driven exploration is proposed by \citep{Burda2019ExplorationBR}. Exploration bonus is based on predicting the output of a fixed and randomly initialized neural network on the next state, given the next state itself. The intuition is that predictive models have low error in states similar to the ones they have been trained on. The method shows better than average human performance on Montezuma’s Revenge.

\textbf{Learning to communicate}~\citep{Mordatch2018EmergenceOG}

Multi-agent game: one agent knows the task and should communicate that task to other agents via broadcast messages. An example of task is to move other agent to the the specific location in the 2D world.

\textbf{DADS}

\citep{Sharma2020DynamicsAwareUD} propose to use an intrinsic reward, which encourages an agent to learn valuable "skills". The intrinsic reward is high if the changes in the environment is predictable for the specific 'skill' and, if the changes in the environment are different for different skills. The separate network is trained to predict environment changes for the specific skill, and its performance is used to calculate intrinsic reward value.

\textbf{World models}~\citep{Ha2018RecurrentWM}

Two models are used: world model and policy model. World model consists of vision compression model V and memory RNN M. V is a variational autoencoder that learns to produce compressed state of a vision input Z. M learns to predict distribution of possible next states $Z_{t+1}$ given its hidden state $H_t$, current state $Z_t$ and action $a_t$. Policy model P receives current state $Z_t$ and current hidden stat $H_t$ and should output a distribution of actions $a_t$ in order to maximize expected reward.

\textbf{Hierarchical Actor-Critic}~\citep{levy2017learning}

First, define Universal MDP. UMDP is just usual MDP with extra set of Goals, denoted as $G$. The transaction probability function doesn't depend on $G$, but agent's policy is. Authors suggest that some goal from $G$ is chosen before game for agent acting in UMPD.

Next, authors make several such agents and label them from $0$ to $k$. $k$ is hyperparameter of model. Every agent from $1$ to $k$ have a set of actions $A$ equal to $S$ or subset of $S$. The lowest level agent number $0$ have action space equal to MDP from initial task.

\textbf{Never-Give-Up intrinsic reward}~\citep{Badia2020NeverGU}

The intrinsic reward is a combination of rewards from two modules: the episodic novelty module and the life-long novelty module.

The life-long novelty module is implemented as the Random Network Distillation~\citep{Burda2019ExplorationBR} based reward.

A self-supervised inverse dynamics model $f$ is used to produce controllable state embeddings $f(x_t)$ The inverse dynamics model is the Siamese net, and is trained to predict the agent action for the pair of consecutive states $x_t$ and $x_{t+1}$. This self-supervised task encourages dropping the parts of the states that are not controllable by the agent, and, hence, are not usable for the action prediction.

The episodic novelty module reward is defined as the inverse of the number of visits of the state, having specific controllable embedding: $n(f(x_t))$. $n(f(x_t))$ is approximated as the sum of similarities between the k nearest controllable embeddings from the episode memory bank $M$. Memory bank is cleared before the start of each episode.

\textbf{Go-explore}~\citep{Ecoffet2019GoExploreAN}

High-dimensional state is represented as low-dimensional cell: (1) without domain knowledge: downsampled cell (low-resolution state image), (2) with domain knowledge: vector of variables hand-crafted from state image.

Learning consists from two phases:

Phase 1: Explore until solved.
\begin{enumerate}
  \item Choose a cell from the archive probabilistically (optionally prefer promising ones, e.g. newer cells)
By explicitly storing a variety of stepping stones in an archive, Go-Explore remembers and returns to promising areas for exploration
  \item Go back to that cell. In a resettable environment, one can simply reset the environment state to that of the cell. Most virtual environments are resettable (e. g. save/load state in empulator).
  \item Explore from that cell (e.g. randomly for n steps)
  \item For all cells visited (including new cells), if the new trajectory is better (e.g. higher score), swap it in as the trajectory to reach that cell
\end{enumerate}

Phase 2: Robustify. Robustify several found solutions (trajectories with high reward) into a deep neural network with an imitation learning algorithm. Imiation learning approach from "Learing from a single demonstration" is used.

\textbf{HER}~\citep{Andrychowicz2017HindsightER}

A Multi-task RL agent gets current goal in addition to the observation. The goal is defined as the target state, that should be achieved. It is nearly impossible to accidental get the positive reward in the large state space. In Hindsight Experience Replay final states of the episodes are considered as target states for synthetic goals. The value function model is trained on both real goals and synthetic goals.

\section{Environments}

\begin{itemize}
  \item MiniGrid~\citep{gym_minigrid}
  \item Classic Atari games~\citep{Mnih2013PlayingAW}. They are deterministic, have discrete actions, use images as an input. Some of the environments e. g. Montezuma's revenge, Pitfall have extremely sparse rewards.
  \item \href{https://minerl.io/competition/}{Minecraft}
  \item \href{http://animalaiolympics.com/AAI/}{Animal AI Testbed} 
  \item ALFRED~\citep{ALFRED20}
\end{itemize}

\bibliographystyle{abbrvnat}
\bibliography{main}

\end{document}
