\documentclass[acmsmall, nonacm]{acmart}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{grffile}

\usepackage{longtable}
\usepackage{subcaption}
\usepackage{xcolor}


% XXX contributors' macros
% gleb
\newcommand{\GG}[1]{\textcolor{red}{[GG: #1]}}
% daniil
\newcommand{\danil}[1]{{\color{red} [#1]}}
% ivan
\newcommand{\attn}[1]{{\color{magenta}{\textbf{#1}}}}


\begin{document}

\title{State description language and PathRL}

\author{Dmitrii Babaev} \thanks{E-mail: dmitri.babaev@gmail.com}
\affiliation{
    \institution{AIRI}
    \institution{Sber AI Lab }
    \country{Moscow, Russia}}

\begin{abstract}
This paper is a work-in-progress description of the PathRL algorithm, an application of the hierarchical reinforcement learning to the arbitrary complex environments with sparse rewards. It includes high-level planning and low-level plan execution.
\end{abstract}

\maketitle

\section{PathRL algorithm}

\subsection{Training scheme}

This goal-setter receives only the current state and produces the set of predicates $e_g = G(s_t)$ that lead to potentially high rewards, i.e. $G$ learns to generate the set of predicates that describe high-reward states. During this phase actions are produced by pre-trained navigator $a_t = N(s_t, G(s_t))$ with frozen parameters, yet differentiable w.r.t. the goal-state embedding. This forces the goal agent $G$ to reproduce and communicate in the same description ''language'' as the encoder agent. This goal-setter is also penalized for the non-reachable goals which worker is failed to achieve.

\subsubsection{Reward scheme}

The goal-setter $G$ is trained with four rewards: the extrinsic reward, i.e. true reward of the environment (\ref{eq-gs-est-reward}); the intrinsic curiosity reward for active exploration (\ref{eq-gs-int-reward}); non-reachable goal penalty, received if the worker is not achieved the generated goal in k steps; and short goals penalty, for goals, achieved in less than c steps (\ref{eq-gs-reward}).

\begin{equation}
\mathrm{in}(s, g) = \mathrm{sim}(E(s),\, g) \ge \sigma
\end{equation}

\begin{equation} \label{eq-gs-reward}
\mathcal{R}_\mathrm{gs}^{(g_i)} = \sum_{t=0..k,\, g=g_i}
\begin{cases}
1 & t > c \land \mathrm{in}(s_t, g) \\
-\epsilon_s & t \le c \land \mathrm{in}(s_t, g) \\
-1 & t=k \land \mathrm{in}(s_t, g) \\
0 & \lnot \mathrm{in}(s_t, g)
\end{cases}
\end{equation}

\begin{equation} \label{eq-gs-est-reward}
\mathcal{R}_\mathrm{ext} = 
\sum_{t \geq 0} \gamma^t r^E_{t+1}
\end{equation}

\begin{equation} \label{eq-gs-int-reward}
\mathcal{R}_\mathrm{int} = 
\sum_{t \ge 0}  \mathrm{surp}(s_{t+1})
\end{equation}

In this scheme $G$ is encouraged to propose queries for states of two kinds: \emph{explotable} states with high extrinsic reward, and \emph{explorable}, unknown ``terrritory'' states high curiosity potential.
%
The combination of curiosity reward scheme and navigator agent have some parallels with Go-explore algorithm~\citep{ecoffet_first_2021}, where the navigation is done via virtual environment resets.

\subsubsection{Navigator training}

The navigator can be pre-trained during preliminary phase, and then, additionally trained during the main phase. The navigator can also be trained from scratch during the main phase, without pre-train during the preliminary phase.

During the navigator training, the navigator weights are unfrozen. The navigator is trained on the intrinsic rewards of reaching the generated goals. The navigator does not receive reward for too complex goals, when goal is not achieved in k steps (\ref{eq-nav-reward}).

\begin{equation} \label{eq-nav-reward}
\mathcal{R}_\mathrm{nav}^{(g_i)}= \sum_{t=0..k,\ g=g_i}
\begin{cases}
1 & \mathrm{in}(s_t, g) \\
-\epsilon & \lnot \mathrm{in}(s_t, g)
\end{cases}
\end{equation}

It is also possible to use a form of navigator pre-train similarly to HER~\citep{andrychowicz_hindsight_2017}. In this setup, the navigator occasionally receives "return to the state" tasks from the state history buffer.

\subsection{Navigation reward}
\label{ssub:navigation_reward}

One possible training scheme is to shape the reward so that it encourages moving closer from the current state $s_t$ to the target state $s_g$. The notion of state \emph{proximity} must take into account the fact that states reside on certain latent manifold, must be consistent with respect to environment's randomness, and at the same time focus on controllable and abstract away uncontrollable or irrelevant features of the states. For example, positively rewarding for diminishing Euclidean distance between $s_g$ and $s_t$ fails all these desirable properties.

Specifically, in Random Disco Maze navigation environment, \citep{badia_never_2020}, the pixel-wise distance would hinder the task completion due to irrelevant random color of the labyrinth's walls. In the same environment the initial and goal states that are separated by an adjacent wall, yet unreachable from one another would be considered close states. Finally, in Pac-Man%
\footnote{
    \url{https://en.wikipedia.org/wiki/Pac-Man_(Atari_2600)}
}
the position of the player within the maze is a controllable feature, while the positions of patrolling ghosts are not, yet the distance between states would be sizeable, even if the protagonist has arrived exactly at the specified target position.
%
Similarly with Freeway%
\footnote{
    \url{https://en.wikipedia.org/wiki/Freeway_(video_game)}
}, the player's car is controllable, while the opponents' cars are not, \citep[fig.~1]{choi_contingency-aware_2019}.

% Another scheme is to reward agent from reaching the state that is similar enough to the target state. In this case, the state similarity function is not required to be transitive.

One possibility is to define a proximity score as $
    d(s_a, s_b) 
        := \frac{\phi(s_a)^\top \phi(s_b)}{\|\phi(s_a)\|\|\phi(s_b)\|}
$ -- the cosine similarity between the latent representations of states. The mapping $
    \phi \colon \mathcal{S} \to \mathbb{R}^{d_h}
$ is a learnt state embedding that removes features irrelevant to the navigator's task and effectively flattens the state manifold. The embedding $\phi$, that drops uncontrollable features can be trained in self-supervised manner on the inverse dynamics task. i.e. inferring the action taken between consecutive states, \citep{choi_contingency-aware_2019,badia_never_2020}.

% In this case the function $d(s_a, s_b)$ that estimates the similarity between states is required. The state similarity function should also be transitive i. e. if $d(s_a, s_b) > d(s_a, s_c)$ then $d(s_b, s_c) > d(s_a, s_c)$.
% XXX transitivity is $a \to b$, $b \to c$ implies $a \to c$.
% XXX the property above fails for euclidean distance: take an acute isosceles triangle.

Another scheme is to split a target state to the overlapping parts in sliding-window manner. Then, it is possible to reward agent if some part of the current state became same as the corresponding part of the target state and penalizie agent if some part was the same and became different. The advantage of this scheme is that the observation encoder is not needed. The main challenge of this scheme is the requirement to generate the whole observation as a goal for a goal-setter. The possible way to simplify this task is to pre-train goal-setter on the pairs of observation from the same episode. I. e. pre-train it on the task to generate the representation of the step t observation taking the representation of the step t-i observation.

\subsection{World state representations}

The most promising possibility, especially in the context of the desiderata outlined in sec.~\ref{ssub:navigation_reward}, is to use compressed abstracted representations of the environment states in the encoder, the navigator and the goal-setter agents. The state embeddings can be produced by a world model pre-trained by the next state prediction task, \citep{ha_recurrent_2018}, by an inverse dynamics model, \citep{badia_never_2020}, or by a contrastive model as in \citep{Ugadiarov2021LongTermEI}.

\subsection{Motivation}

% XXX oof, i will reword this later.
The proposed scheme should be effective in sparse extrinsic reward settings: the low-level basic interaction in the environment, when trained to solve the environment on its own, would experience the detrimental effects of reward sparsity most acutely. However, if instead we train a low level agent on state-navigation tasks, motivating it for achieving the goals (a sort of synthetic intrinsic reward, but not curiosity-based), and delegate the solution of the environment itself to an abstract planning agent, which collects extrinsic rewards, we could effectively significantly reduce the planer's reward sparsity. If the high-level agent uses relatively small set of high-level actions (goals) then the number of possible trajectories to explore is drastically reduced: the trajectories of 10 steps of high-level actions will be used instead of trajectories of 100 steps of the low-level actions.

By using manager-worker scheme we are able to learn efficient reliable navigation skills using synthetic navigation tasks, 
without any extrinsic reward.
%
The proposed scheme should be effective for the multi-task learning setting in the same environment: a well trained and capable navigator model, even though trained for one specific environment and goal state distribution, can be effectively reused across different tasks.

As shown by \citet{ecoffet_first_2021} the exploration of distant states greatly improves the algorithm effectiveness. The availability of navigator agent enables training a goal agent, which is encouraged to explore remote states.
%
% XXX i am afraid this contribution is more of \citet{badia_never_2020,choi_contingency-aware_2019} rather than ours.
In contrast to \citep{ecoffet_first_2021}, though, by using learnt state representations that abstract away irrelevant information, and by decoupling goal-setting from goal-achieving the scheme proposed in this work may be applied to stochastic and multi-task environments as well.
% XXX ``virtual environment resets'' are useful for faster training and more diverse
%  dataset, their crude state discretization ``hack'', however, is not.

\subsection{Interpretable RL}

The proposed PathRL algorithm can be made interpretable by adding state representation decoder, that would transform a goal back to the desired state, associated with that goal. This can be done training state decoder together with state encoder during the first phase of training.

\subsubsection{Discriminator loss for goal-setter}

To force goal-setter to generate goals, that are opservation representations, additional losses can be used. One option is to use discriminator loss. Separate discriminator net can be trained to distinguish between the generated goals and state representations and its accuracy can be used as loss to goal-setter similarly to the GAN training scheme (\ref{eq-disc-loss}). Another option is to pre-train goal-setter on the pairs of observation from the same episode. I. e. pre-train it on the task to generate the representation of the step t observation taking the representation of the step t-i observation.

\begin{equation} \label{eq-disc-loss}
min_{\Theta_M} max_{\Theta_D} L(\Theta_M, \Theta_D) = E_{s \in P_s} \log D_{\Theta_D}(E(s)) + E_g \log(1 - D_{\Theta_D}(M_{\Theta_M}(s)))
\end{equation}

\subsubsection{Why goal-setter goals can provide interpretation}

Ideally trained goal-setter should output goals that are keypoint states. The keypoint state of the pair of states $s_b$ and $s_e$, is the state, that can should be included to any trajectory between $s_b$ and $s_e$. The keypoint state of some environment is the state with high \emph{betweenness} measure, i. e. the state that should be included in many possible trajectories for this environment.

\section{Semantic observation attributes}

The alternative, and possibly more scaleable and interpretable approach to PathRL is to use the set of facts about the observation instead of its representation. Each fact is defined as a \href{https://en.wikipedia.org/wiki/Semantic_triple}{semantic triple} (subject, predicate and object), where first and third terms are taken form the same fixed-size set of possible tokens A and second term is taken from another fixed-size set of possible tokens B.

On the first phase the attribute \textit{difference} between the current observation and some random observation is calculated. A random subset of attributes from that difference is selected and passed to the worker. The worker goal is to reach any state which have the passed set of attributes. On the second phase the task of goal generator is to produce the subset of attributes that would maximize the reward.

Semantic observation attributes can be manually engineered, or possibly mined from the observations. Some environments. e. g. UI task environments, like MiniWob++~\cite{liu2018reinforcement} or computer game environments, like NetHack~\cite{kuttler_nethack_2020} already have highly structured states, that can be used as the replacement of the fact sets.

If the observations are 2D images, it is reasonable to split them using some grid and define an unidependent set of semantic attributes for each cell of the grid. This would allow to preserve the spatial information from the original observation.

\subsection{Image guessing}

Observation attributes can possibly be mined using approach considered in language emergence papers. In this scenario the image attributes is learned in the unsupervised setting of image guessing game similarly to~\citet{Havrylov2017EmergenceOL}. In~\citet{Havrylov2017EmergenceOL} the image is transformed into the sequence of tokens, in PathRL, the image is transfromed to the set of facts.

\subsection{Discrete world model}

The possible approach to extract state entities is to train the global state encoder to produce a fixed set of the discrete vectors or global vectors. The global vectors will be used for the restoration of the entire sequience of states along some trajectory of the state-action pairs. The global vectors are produced by encoding the initial state of the trajectory. The VQ-VAE like approach can be used to make global vectors discrete.

The state restoration model is the recurrent network, that, on each step, takes the global vectors and the current action and outputs the next state prediction. The initial hidden vector of the RNN is produced from the initial state by the dedicated hidden vector encoder. The state difference loss is used to train the global state encoder, the state restoration RNN, and the initial hidden vector encoder in the end2end manner. The system does not take the environment states as its input, except the initial state.

\subsubsection{Discrete space autoencoder}

Another option is to introduce a sort of a spatial prior and, instead of predicting the next state, predict the current state in the autoencoding manner. In this case, the current state encoder outputs the matrix of the categorical distributions over the global vectors. In this case, each cell of the output contains an attention vector, that will be applied to calculate the weighted sum the global vectors. A form of the dot-product attention can be used: the dot products of the output vector of the current cell and each of the global vectors, normalized via the softmax will be used as the weights of the global vectors. Finally, matrix of the weigthed sums of the global vectors is used to produce the current state prediction. Here, the trajectory is used as the source of a different combinations of the same entities, the actions are not used at all.

\subsection{Search in semantic observation space}

The perespective approach is to use neural net guided MCTS to find the valuable combination of semantic attributes. The desired change in any single observation attribute can be used as a high-level action, and will form a branch of the semantic observation tree. In this setup, the goal-setter iteratively proposes changes in the attributes, and builds a search tree to find a high reward combination of semantic attributes. The navigator task is to reach the proposed high-level state, i. e. implement the proposed change in the attributes.

The desired change in a single observation attribute may also lead to inevitable changes in other attributes, but for the tree search algorithm it is practical to define an action by the single changed attribute.

As in AlphaZero~\cite{silver2017mastering} the additional value estimator for the proposed changes in the attributes can be used to guide the search. It takes the updated set of the semantic attributes, associated with the new branch of the semantic tree and returns its accessed value.

The overall algorithm respembles MuZero~\cite{schrittwieser2020mastering}, but, instead of state space, the search is made in the semantic attributes space, and attribute changes are used instead of basic actions. Also, in the basic version, the algorithm interacts with the actual environment instead of the learned model.

The MCTS search can be guided by the following signals: negative penalty if the navigator can not reach the proposed state in k steps, the reward for reaching the desired state, e. g. achieving the assigned task, or winnning the game, the negative reward for reaching the undesired state, e. g. losing the game. In some environments there can be only desired state, on others, only undesired states exist, still, only one kind of the states can be used to guide the search.

\section{Related work}
\label{sec:related_work}

\paragraph{Feudal RL by~\citet{Dayan1992FeudalRL}} % (fold)
\label{par:feudal_rl}

The task is to find a way in the maze. A hierarchy of policies is defined. A policy use blocks of space as atomic movement positions. Each level of hierarchy use exponentially larger blocks comparing to the previous level. A policy selects a desired block to move to as its action and then assigns a task to lower-level policy. Higher-level policy defines reward for the lower-level policy. The reward for lower-level policy is gained if it successfully guides the agent to the block, defined by the higher-level policy.

% paragraph feudal_rl (end)

\paragraph{STRAW by~\citet{Vezhnevets2016StrategicAW}} % (fold)
\label{par:straw}

The network holds two structures in memory: action plan A and commitment plan c. T-th column of A contains probabilities to perform specific action at time t. c is a vector of probabilities to update the current plan at time t. If $g \sim c_{t-1} = 1$ then the network performs plan update. Both A and c are refreshed during update. The network is trained using A3C.

% paragraph straw (end)

\paragraph{Hierarchical Deep Reinforcement Learning by~\citet{Kulkarni2016HierarchicalDR}} % (fold)
\label{par:hierarchical_deep_rl}

The meta-controller learns an approximation of the optimal goal policy $\pi(g|s)$, i. e. it receives state s and select a goal g from the set of all possible current goals G. The meta-controller operates on the slower time-scale than the controller. The meta-controller receives extrinsic reward from the environment.

The controller learns an approximation of the optimal action policy $\pi(a|g,s)$, i. e. it receives state s and goal g and selects an action a. The internal critic provides intrinsic reward for the controller. For example, a critic for a game can check that specific conditions are fulfilled, e. g. an agent reaches the door.

The critic and the set of possible goals G are not learned but are considered available.

% paragraph hierarchical_deep_rl (end)

\paragraph{HIRO by~\citet{Nachum2018DataEfficientHR}} % (fold)
\label{par:hiro}

Ant Gather / Ant Maze / Ant Push / Ant Fall tasks are used. The higher-level policy instructs the lower-level policy via high-level actions, or goals, which it samples anew every c steps. A higher-level policy takes current state and outputs the goal for lower-level policy. The lower-level policy interacts directly with the environment. The goal simply instructs the lower-level policy to reach the specific position in the 2D maze. The reward for lower-level policy is provided if the environment yields an observation close to matching the desired goal.

% paragraph hiro (end)

\paragraph{DADS by~\citet{Sharma2020DynamicsAwareUD}} % (fold)
\label{par:dads}

The authors propose an intrinsic reward, which encourages an agent to learn valuable ``skills''. The intrinsic reward is high if the changes in the environment is predictable for the specific 'skill' and, if the changes in the environment are different for different skills. The separate network is trained to predict environment changes for the specific skill, and its performance is used to calculate intrinsic reward value.

% paragraph dads (end)

\paragraph{Hierarchical Actor-Critic by~\citet{levy2018hierarchical}} % (fold)
\label{par:hierarchical_ac}

%First, define Universal MDP. UMDP is just usual MDP with extra set of Goals, denoted as $G$. The transaction probability function doesn't depend on $G$, but agent's policy is. Authors suggest that some goal from $G$ is chosen before game for agent acting in UMPD.
% XXX think about the multi-criteria optimization method by \citet{dosovitskiy_you_2019}

%Next, authors make several such agents and label them from $0$ to $k$. $k$ is hyperparameter of model. Every agent from $1$ to $k$ have a set of actions $A$ equal to $S$ or subset of $S$. The lowest level agent number $0$ have action space equal to MDP from initial task.

The authors propose multi-level hierarchical RL where environment states are used as sub-goals. I. e. all agents except the the lowest level agent have the set of environment states as their action space. The lowest level agent have the action space of the initial task. The lower-level agents are trained on the goal states, selected from the past experience similarly to the HER~\cite{andrychowicz_hindsight_2017} approach.

% paragraph hierarchical_ac (end)

\paragraph{Go-explore approach by~\citet{ecoffet_first_2021}} % (fold)
\label{par:go-explore}

High-dimensional state is mapped into a low-dimensional space, with potentially high number of collisions, in order to group similar states together:
\begin{itemize}
    \item without domain knowledge: state projected into a low-dimensional grid (low-resolution state image)
    \item with domain knowledge: vector of hand-crafted features from state image
\end{itemize}

Learning comprises two phases:
\begin{enumerate}
    \item Phase~1: Explore until solved
    \begin{enumerate}
      \item Choose a cell from the archive probabilistically (optionally prefer promising ones, e.g. cells with less number of visits)
    By explicitly storing a variety of stepping stones in an archive, Go-Explore remembers and returns to promising areas for exploration
      \item Go back to that cell. In a resettable environment, one can simply reset the environment state to that of the cell. Most virtual environments are resettable (e. g. save/load state in empulator).
      \item Explore from that cell (e.g. randomly for $n$ steps)
      \item For all cells visited (including new cells), if the new trajectory is better (e.g. higher score), swap it in as the trajectory to reach that cell
    \end{enumerate}

    \item Phase~2: Robustification. Robustify several found solutions (trajectories with high reward) into a deep neural network with an imitation learning algorithm. Imitation learning approach from "Learning from a single demonstration" is used.
    % https://ieeexplore.ieee.org/document/614389
    % https://openreview.net/forum?id=OOTYUAIyYT
\end{enumerate}

% paragraph go-explore (end)

\paragraph{Hindsight Experience Replay by~\citet{andrychowicz_hindsight_2017}} % (fold)
\label{par:hindsight}

A Multi-task RL agent gets current goal in addition to the observation. The goal is defined as the target state, that should be achieved. It is nearly impossible to accidental get the positive reward in the large state space. In Hindsight Experience Replay final states of the episodes are considered as target states for synthetic goals. The value function model is trained on both real goals and synthetic goals.

% paragraph hindsight (end)

\paragraph{FuNs by~\citet{Vezhnevets2017FeUdalNF}} % (fold)
\label{par:funs}

The hierarchy of two agents is used: the master agent produces the goal vector and the worker agent produces actions. Worker agent is trained mostly on the intrinsic reward for moving in the goal direction inside state space.

% paragraph funs (end)

\paragraph{GoCu by~\citet{Bougie2019SkillbasedCF}} % (fold)
\label{par:gocu}

The intrinsic curiosity reward for a state $s_t$ defined as minus probability of reaching some pre-defined state, called goal. The reward approaches zero as probability of reaching a goal approaches one. Several random goal states are selected from the memory bank before start of the episode. The maximum probability of the probabilities of reaching the selected goals is used to calculate the intrinsic reward. The separate predictor is trained to estimate the probability of reaching a goal from the current state. Latent representations of the environment states are used, the states are embedded to the latent space by using variational auto-encoder.

% paragraph gocu (end)

\paragraph{SPTM by~\citet{savinov2018semiparametric}} % (fold)
\label{par:sptm}

A Multi-task RL agent gets current goal in addition to the observation. The goal is defined as a state of the environment. Two modules are used: the waypoint selection module and the locomotion module.

The waypoint selection consists from retrieval network R and the memory graph. The R outputs the measure of closeness between two observations, and is trained on binary classification task, where positive class is defined as a pair of observations with small action distance.

The memory graph's nodes are observations and an edge is set if two observations of different nodes are close enough according to R. During the execution phase, the node closest to the current observation and the node closest to the goal state are selected using R. The waypoint node is the node that lies on the shortest path between the current observation node and goal node and which is also close enough to the current observation according to R.

The locomotion module is implemented as the network L which takes current observation and waypoint observation and outputs the current action.

Both R and L are trained in self-supervised manner on the trajectories of the random agent.

% paragraph sptm (end)

\paragraph{Episodic Curiosity Through Reachability by \citet{savinov2018episodic}}

The authors propose to use minimum number of steps required to reach a state from some other state as the curiosity reward. The embeddings of the states with high curiosity reward are stored in the episodic memory and are used to estimate the reward. The compactor network is trained to estimate the number of steps between states and takes a pair of state embeddings as its input.

% paragraph ectr (end)

\paragraph{Automatic Goal Generation by \citet{pmlr-v80-florensa18a}}

The authors consider the goal achievement task which can be described as the task to reach the specific embedding of an observation. They propose to use a goal generator implemented as a GAN which generate the goals which are similar in complexity to the previous iteration goals. The goals are generated to be increasingly challenging for the policy. In the experiments, the authors consider 2D embeddings of the Ant tasks as the goals.

% paragraph ectr (end)

\paragraph{Program Synthesis Guided RL by \citet{Yang2021ProgramSG}}

The authors propose three-component system: The Hallucinator g is a generative world model. The Synthesizer $\pi$ uses Hallucinator to explore paths in the imaginary world which would satisfy the desired goal $\phi$, the goal is explicitly defined by a user.

The Synthesizer produces a program p consisting from k components $\{c\}_1^k$. Each component $c_t$ contains some condition $\beta_t$ and some policy $\pi_t$ which says to execute policy $\pi_t$ until condition $\beta_t$ holds. A user should provide a set of prototype components $\tilde{c}$, where $\tilde{c}$ is a logical formula which defines some useful subtask. The policy for $\tilde{c}$ is trained using RL.

Finally, the Executor executes p generated by $\pi$ for N steps. If the desired goal $\phi$ is not satisfied, then the above process is repeated.

% paragraph ectr (end)

\paragraph{RIG by \citet{Nair2018VisualRL}}

Goal-oriented RL scenario is considered. The goal is defined as a state to reach. The authors propose to use VAE to generate the latent representations of the states.

The reward is defined as the euclidean distance between the latent states. Minimizing this distance in the VAE latent space is equivalent to rewarding reaching states that maximize the probability of the latent goal. This is not true for normal autoencoders since distances are not trained to have any probabilistic meaning.

It is possible to sample new latent states from VAE, and, hence, sample new goals. By utilizing goal sampling it is possible to generate infinitely many goals and rewards for any observed state, which can be then used to train Q function estimator.

% paragraph rig (end)

\paragraph{Abstract Models by \citet{liu2019learning}}

Authors propose to to use manager-worker scheme where manager work in the space an abstract states which is substantially smaller than the space of actual states. Manager action space is the space of transitions between the abstract states. The new absract state is  added if the discovered state is not assigned to any existing abstract state. The worker is trained to reliably navigate between the pairs of abstract states. The exploration is performed by selecting an abstract state with low number of exploration attempts. The hand crafted-function to capture the grid cell of the agent is used to produce the abstract state.

% paragraph abstract (end)

\paragraph{Consciousness-Inspired Planning Agent by \citet{Zhao2021ACP}}

The authors propose to use set-based representations of the observations instead of vector-based. The approach to produce state-based representations is taken from object detection work by \citet{Carion2020EndtoEndOD}. The authors also propose to limit the objects used by world model by introducing the bottleneck, i. e. only a small subset of observation's objects is passed to the world model and returned by it. The returned subset of objects is integrated back to the observation to produce the predicted observation of the next state. The world model is used in the tree search planning algorithm to select most promising action of the agent.

% paragraph consciousness (end)

\paragraph{RbExplore by \citet{Ugadiarov2021LongTermEI}}

The authors propose a version of Go-explore~\citep{ecoffet_first_2021} exploration algorithm. They propose to use learned state embeddings instead of image upscaling. The state embeddings are learned in unsupervised manner using the closeness on the trajectory as similarity.

% paragraph rbexplore (end)

\paragraph{GMHRL by \citet{chen2019learning}}

The authors propose a manager-worker framework, similar to PathRL, where the worker is trained on the state achievement task. The master is trained to generate potentially valuable states using the extrinsic reward. The buffer of observed states is used to train worker on the attainable goals. The algorithm is tested in the multi-task MiniGrid environment, where the current task is explicitly passed to the manager network.

% paragraph gmhrl (end)

\paragraph{Adversarially Guided Subgoals by \citet{https://doi.org/10.48550/arxiv.2201.09635}}

Two-level hierarchy with manager and worker agents is used. Environment states are used as action space for the manager. Additionally, a discriminator net is trained to distinguish goals, generated by manager from the observed states. Discriminator provides additional GAN loss for the manager training.

% paragraph ags (end)

\paragraph{Deep Hierarchical Planning by \citet{https://doi.org/10.48550/arxiv.2206.04114}}

The authors propose manager-worker agent scheme, which is similar to PathRL. Instead of raw states all components of the system operate on the latent state representations, produced by the PlaNet word model. The goal VQ-VAE is used to compresses state vectors into vectors of discrete codes. Manager selects a discrete code, that is then passed to the decoder, which restores it to the goal vector.

The goal vectors are restored to images by the additional decoder, trained along with the world model.

% paragraph dhp (end)

\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
